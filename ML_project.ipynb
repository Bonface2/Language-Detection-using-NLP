{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmoxqFeJiZCq"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ndn4A4i0H9"
      },
      "source": [
        "Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bLGofsbix0L",
        "outputId": "ebb3cac1-3f12-4043-a16c-42fc8f4c05c7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwLlxQUSp8VV"
      },
      "source": [
        "data = pd.read_csv('../content/gdrive/MyDrive/sentences.csv', sep='\\t', \n",
        "                            encoding='utf8', \n",
        "                            index_col=0,\n",
        "                            names=['lang','text'])\n",
        "data.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTQBnAHirV87"
      },
      "source": [
        "Filter Data by Text Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywzbpp_vrZt5"
      },
      "source": [
        "len_cond = [True if 20<=len(s)<=200 else False for s in data['text']]\n",
        "data = data[len_cond]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwPJVnrfr-Mx"
      },
      "source": [
        "Filter by Text Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co0HWdCKsAod"
      },
      "source": [
        "lang = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
        "data = data[data['lang'].isin(lang)]\n",
        "data.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFpXvYbpsNed"
      },
      "source": [
        "Select at most 50,000 rows per language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqG57Z-LsQKl"
      },
      "source": [
        "data_trim = pd.DataFrame(columns=['lang','text'])\n",
        "for l in lang:\n",
        "    lang_trim = data[data['lang'] ==l].sample(50000,random_state = 100)\n",
        "    data_trim = data_trim.append(lang_trim)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg9mGBhnupW7"
      },
      "source": [
        "Split Data into Training, Testing and Validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRsyZXlquvUP"
      },
      "source": [
        "data_shuffle = data_trim.sample(frac=1)\n",
        "\n",
        "train = data_shuffle[0:210000]\n",
        "valid = data_shuffle[210000:270000]\n",
        "test = data_shuffle[270000:300000]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CkXeXGw_X_x"
      },
      "source": [
        "Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NK9VUMH_aAe"
      },
      "source": [
        "Function to get 200 most common trigrams from each of the 6 languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWWjfJeK_dn-"
      },
      "source": [
        "def get_trigrams(corpus,n_feat=200):\n",
        "    \n",
        "    #fit the n-gram model\n",
        "    vectorizer = CountVectorizer(analyzer='char',ngram_range=(3, 3) ,max_features=n_feat)\n",
        "    \n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    \n",
        "    #Get model feature names\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "    \n",
        "    return feature_names"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVnoW7v0_9qo"
      },
      "source": [
        "Use get_triagrams function to obtain the 200 common trigrams from each language and add them to a set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEchPaaIAEpf"
      },
      "source": [
        "features = {}\n",
        "features_set = set()\n",
        "\n",
        "for l in lang:\n",
        "    \n",
        "    #get corpus filtered by language\n",
        "    corpus = train[train.lang==l]['text']\n",
        "    \n",
        "    #get 200 most frequent trigrams\n",
        "    trigrams = get_trigrams(corpus)\n",
        "    \n",
        "    #add to dict and set\n",
        "    features[l] = trigrams \n",
        "    features_set.update(trigrams)\n",
        "\n",
        "    \n",
        "#create vocabulary list using feature set\n",
        "vocab = dict()\n",
        "for i,f in enumerate(features_set):\n",
        "    vocab[f]=i   "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev4X8emuFXyw"
      },
      "source": [
        "Vectorize Sentences in the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS3FIuwvFbbW"
      },
      "source": [
        "#train count vectoriser using vocabulary\n",
        "vectorizer = CountVectorizer(analyzer='char',\n",
        "                             ngram_range=(3, 3),\n",
        "                            vocabulary=vocab)\n",
        "\n",
        "#create feature matrix for training set\n",
        "corpus = train['text']   \n",
        "X = vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "train_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
        "\n",
        "#We end up with a feature matrix"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyHi99xtGLwW"
      },
      "source": [
        "Use min-max scaling to scale our feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjQVTyOGGO3b"
      },
      "source": [
        "train_min = train_feat.min()\n",
        "train_max = train_feat.max()\n",
        "train_feat = (train_feat - train_min)/(train_max-train_min)\n",
        "\n",
        "#Add target variable \n",
        "train_feat['lang'] = list(train['lang'])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEV81_LuKZzr"
      },
      "source": [
        "Validate and Vectorize Testing and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5X615S3KedV"
      },
      "source": [
        "#create feature matrix for validation set\n",
        "corpus = valid['text']   \n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "valid_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
        "valid_feat = (valid_feat - train_min)/(train_max-train_min)\n",
        "valid_feat['lang'] = list(valid['lang'])\n",
        "\n",
        "#create feature matrix for test set\n",
        "corpus = test['text']   \n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "test_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
        "test_feat = (test_feat - train_min)/(train_max-train_min)\n",
        "test_feat['lang'] = list(test['lang'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkikieCtLUvW"
      },
      "source": [
        "Encode the data in readiness for training using label encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DId9xuULc2E"
      },
      "source": [
        "#Fit encoder\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(['deu', 'eng', 'fra', 'ita', 'por', 'spa'])\n",
        "\n",
        "def encode(y):\n",
        "    \"\"\"\n",
        "    Returns a list of one hot encodings\n",
        "    Params\n",
        "    ---------\n",
        "        y: list of language labels\n",
        "    \"\"\"\n",
        "    \n",
        "    y_encoded = encoder.transform(y)\n",
        "    y_dummy = np_utils.to_categorical(y_encoded)\n",
        "    \n",
        "    return y_dummy"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiN7-tUxMsr9"
      },
      "source": [
        "Train DNN. We have 3 hidden layers, each with 500, 250 and 250 nodes respectively. The output layer has 6 nodes. Relu used for activating the hidden layers whereas the output layer is activated using softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EN4z0maNO_0",
        "outputId": "50b4e446-ca04-41ba-82c4-c8a5c1c33789"
      },
      "source": [
        "#Get training data\n",
        "x = train_feat.drop('lang',axis=1)\n",
        "y = encode(train_feat['lang'])\n",
        "\n",
        "#Define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=664, activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Train model\n",
        "model.fit(x, y, epochs=4, batch_size=100)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "2100/2100 [==============================] - 31s 14ms/step - loss: 0.0729 - accuracy: 0.9755\n",
            "Epoch 2/4\n",
            "2100/2100 [==============================] - 30s 14ms/step - loss: 0.0342 - accuracy: 0.9878\n",
            "Epoch 3/4\n",
            "2100/2100 [==============================] - 30s 14ms/step - loss: 0.0203 - accuracy: 0.9928\n",
            "Epoch 4/4\n",
            "2100/2100 [==============================] - 30s 14ms/step - loss: 0.0117 - accuracy: 0.9960\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe57d679790>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8mPnu8QOnDS"
      },
      "source": [
        "Get Predictions and accuracy on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToaKa5MuOpWJ",
        "outputId": "adf6e5d5-fadc-42ac-d0f8-b78ca6274ef8"
      },
      "source": [
        "x_test = test_feat.drop('lang',axis=1)\n",
        "y_test = test_feat['lang']\n",
        "\n",
        "#Get predictions on test set\n",
        "labels = model.predict(x_test)\n",
        "classes_x=np.argmax(labels,axis=1)\n",
        "predictions = encoder.inverse_transform(classes_x)\n",
        "\n",
        "#Accuracy on test set\n",
        "accuracy = accuracy_score(y_test,predictions)\n",
        "print(accuracy)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9865666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}